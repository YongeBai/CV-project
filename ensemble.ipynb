{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_path = \"datasets\"  # change to your own download path\n",
    "path_query = download_path + \"/query_img_4186\"\n",
    "path_query_txt = download_path + \"/query_img_box_4186\"\n",
    "\n",
    "# path_query_txt is the directory to the bounding box information of the instance(s) for the query images\n",
    "path_gallery = download_path + \"/gallery_4186\"\n",
    "\n",
    "name_query = glob.glob(path_query + \"/*.jpg\")\n",
    "num_query = len(name_query)\n",
    "\n",
    "name_box = glob.glob(path_query_txt+\"/*.txt\")\n",
    "\n",
    "name_gallery = glob.glob(path_gallery + \"/*.jpg\")\n",
    "num_gallery = len(name_gallery)\n",
    "record_all = np.zeros((num_query, len(name_gallery)))\n",
    "\n",
    "query_imgs_no = [x.split(\"/\")[-1][:-4] for x in glob.glob(path_query + \"/*.jpg\")]\n",
    "gallery_imgs_no = [x.split(\"/\")[-1][:-4] for x in glob.glob(path_gallery + \"/*.jpg\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer encoder\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "import torch.nn as nn\n",
    "from resnet import ResNet50\n",
    "\n",
    "\n",
    "PATH = \"./resnet50.pth\"\n",
    "resnet = ResNet50(10)\n",
    "resnet.load_state_dict(torch.load(PATH, map_location=torch.device('cpu')))\n",
    "resnet.fc = nn.Sequential()\n",
    "\n",
    "resnet.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "\n",
    "\n",
    "sam = timm.create_model(\n",
    "    'samvit_large_patch16.sa1b',\n",
    "    pretrained=True,\n",
    "    num_classes=0, \n",
    ")\n",
    "sam = sam.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Resize, Compose, ToTensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class QueryDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_paths, bounding_box_path,transform):        \n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "        self.bounding_box_path = bounding_box_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        bounding_box = self.bounding_box_path[idx]\n",
    "        x, y, w, h = np.loadtxt(bounding_box)\n",
    "\n",
    "        # image = image.crop((x, y, x+w, y+h))        \n",
    "        image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "class GalleryDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_paths, transform):        \n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path)        \n",
    "        image = self.transform(image)\n",
    "        return image  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "cnn_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "data_config = timm.data.resolve_model_data_config(sam)\n",
    "sam_transforms = timm.data.create_transform(**data_config, is_training=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_query_dataset = QueryDataset(name_query, name_box, transform=cnn_transforms)\n",
    "cnn_query_dataloader = DataLoader(cnn_query_dataset, batch_size=20, num_workers=32)  \n",
    "\n",
    "cnn_query_embeddings = []\n",
    "with torch.no_grad():\n",
    "    for images in cnn_query_dataloader:\n",
    "        outputs = resnet(images)\n",
    "        cnn_query_embeddings.append(outputs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_gallery_dataset = GalleryDataset(name_gallery, transform=cnn_transforms)\n",
    "cnn_gallery_dataloader = DataLoader(cnn_gallery_dataset, batch_size=2048, num_workers=32)  \n",
    "\n",
    "cnn_gallery_embeddings = []\n",
    "with torch.no_grad():\n",
    "    for images in cnn_gallery_dataloader:\n",
    "        outputs = resnet(images)\n",
    "        cnn_gallery_embeddings.append(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tformer_query_dataset = QueryDataset(name_query, name_box, transform=sam_transforms)\n",
    "tformer_query_dataloader = DataLoader(tformer_query_dataset, batch_size=20, num_workers=32)  \n",
    "\n",
    "tformer_query_embeddings = []\n",
    "with torch.no_grad():\n",
    "    for images in tformer_query_dataloader:\n",
    "        # images = images.to(device)        \n",
    "        outputs = sam.forward_features(images)\n",
    "        tformer_query_embeddings.append(outputs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tformer_gallery_dataset = GalleryDataset(name_gallery, transform=sam_transforms)\n",
    "tformer_gallery_dataloader = DataLoader(tformer_gallery_dataset, batch_size=512, num_workers=32)\n",
    "\n",
    "tformer_gallery_embeddings = []\n",
    "with torch.no_grad():\n",
    "    for images in tqdm(tformer_gallery_dataloader, desc=\"progress\"):\n",
    "        outputs = sam.forward_features(images)\n",
    "        tformer_gallery_embeddings.append(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(tformer_gallery_embeddings, \"gallery_embeddings_SAM.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_query_embeddings_combined = torch.cat(cnn_query_embeddings, dim=0)\n",
    "cnn_gallery_embeddings_combined = torch.cat(cnn_gallery_embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_query_embeddings_combined = cnn_query_embeddings_combined.view(cnn_query_embeddings_combined.shape[0], -1)\n",
    "cnn_gallery_embeddings_combined = cnn_gallery_embeddings_combined.view(cnn_gallery_embeddings_combined.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cnn_query_embeddings_combined.shape)\n",
    "print(cnn_gallery_embeddings_combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tformer_query_embeddings_combined = torch.cat(tformer_query_embeddings, dim=0)\n",
    "tformer_gallery_embeddings_combined = torch.cat(tformer_gallery_embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tformer_query_embeddings_combined.shape)\n",
    "print(tformer_gallery_embeddings_combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tformer_query_embeddings_combined = tformer_query_embeddings_combined.view(tformer_query_embeddings_combined.shape[0], -1)\n",
    "tformer_gallery_embeddings_combined = tformer_gallery_embeddings_combined.view(tformer_gallery_embeddings_combined.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "tformer_query_embeddings_norm = F.normalize(tformer_query_embeddings_combined)\n",
    "tformer_gallery_embeddings_norm = F.normalize(tformer_gallery_embeddings_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tformer_query_embeddings_norm.shape)\n",
    "print(tformer_gallery_embeddings_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dot_product_matrix = torch.matmul(tformer_query_embeddings_norm, tformer_gallery_embeddings_norm.T)\n",
    "\n",
    "query_norms = torch.norm(tformer_query_embeddings_norm, dim=1, keepdim=True)\n",
    "gallery_norms = torch.norm(tformer_gallery_embeddings_norm, dim=1, keepdim=True)\n",
    "\n",
    "sam_cosine_similarities = dot_product_matrix / (query_norms * gallery_norms.T)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sam_cosine_similarities.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_cosine_similarities = torch.load(\"cosine_similarity_SigLIP.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_weight = 0.1\n",
    "sam_weight = 0.5\n",
    "clip_weight = 0.4\n",
    "\n",
    "weighted_similarities = tformer_weight*tformer_cosine_similarities + cnn_weight*cnn_cosine_similarities+clip_weight*clip_cosine_similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_similarities, sorted_indices = torch.sort(weighted_similarities, dim=1, descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"ensemble_rank_list.txt\", \"w\")\n",
    "for i in range(len(sorted_indices)):\n",
    "    f.write(\"Q\" + str(i + 1) + \": \")    \n",
    "    f.write(\" \".join([str(x.item()) for x in sorted_indices[i]]))\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "I/O operation on closed file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(query_img_no)):\n\u001b[1;32m     28\u001b[0m     top_10_indices \u001b[38;5;241m=\u001b[39m sorted_indices[query_imgs_no\u001b[38;5;241m.\u001b[39mindex(query_img_no), :\u001b[38;5;241m10\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m---> 29\u001b[0m     \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mQ\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtop_10_indices\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: I/O operation on closed file."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "            \n",
    "download_path = \"datasets\"\n",
    "path_gallery = os.path.join(download_path, \"gallery_4186\")\n",
    "\n",
    "for query_img_no in query_imgs_no:\n",
    "    # Create a folder for the query image\n",
    "    query_img_folder = os.path.join(download_path, f\"image_{query_img_no}\")\n",
    "    os.makedirs(query_img_folder, exist_ok=True)\n",
    "    \n",
    "    # Get the top 10 indices for this query image\n",
    "    top_10_indices = sorted_indices[query_imgs_no.index(query_img_no), :10].tolist()\n",
    "    \n",
    "    # Copy the top 10 most similar gallery images to the query image's folder\n",
    "    for index in top_10_indices:\n",
    "        gallery_img_name = gallery_imgs_no[index] + \".jpg\" # Assuming gallery images are in .jpg format\n",
    "        src_path = os.path.join(path_gallery, gallery_img_name)\n",
    "        dst_path = os.path.join(query_img_folder, gallery_img_name)\n",
    "        shutil.copy(src_path, dst_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1258',\n",
       " '1656',\n",
       " '1709',\n",
       " '2032',\n",
       " '2040',\n",
       " '2176',\n",
       " '2461',\n",
       " '27',\n",
       " '2714',\n",
       " '316',\n",
       " '35',\n",
       " '3502',\n",
       " '3557',\n",
       " '3833',\n",
       " '3906',\n",
       " '4354',\n",
       " '4445',\n",
       " '4716',\n",
       " '4929',\n",
       " '776']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_imgs_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ensemble_top10.txt\", 'w') as f:\n",
    "    for i in range(len(query_imgs_no)):\n",
    "        top_10_indices = sorted_indices[i, :10].tolist()\n",
    "        f.write(f\"Q{i+1}: {str(top_10_indices)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
